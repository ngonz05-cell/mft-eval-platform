# MFT Eval Template - Quick Start
# Copy this template to create a new eval

name: [your_eval_name]
version: "1.0.0"
team: [your_team]

owner:
  pm: "@your_pm"
  eng: "@your_eng"

description: |
  [What capability does this eval measure?]
  [What problem is this eval validating?]

capability:
  what: "[Precise behavior being tested]"
  why: "[Why this matters for users/business]"

dataset:
  source: "[hive://path or gsheet://url or csv://local_path]"
  size: [number of examples, start with 50-100 for MVE]

scoring:
  primary_metric: [accuracy/f1/weighted_score/custom]
  metrics:
    - name: [metric_name]
      type: [exact_match/f1_score/token_f1/numeric_tolerance/llm_judge]
      weight: [0.0 to 1.0, should sum to 1.0]

thresholds:
  baseline:
    [metric_name]: [value, e.g., 0.80 for MVE]
  target:
    [metric_name]: [value, e.g., 0.95 for production]
  blocking: [true/false]

automation:
  schedule: "[cron expression or 'manual']"
  ci_integration: [true/false]
  alert_on_regression: [true/false]
  alert_channel: "[#your-slack-channel]"

tags:
  - [tag1]
  - [tag2]

status: draft  # draft, active, or deprecated

# =============================================================================
# VALIDATION CHECKLIST (from reference doc)
# =============================================================================
# Before activating this eval, ensure you can answer:
#
# ☐ Eval Name: What capability does this measure?
# ☐ User/System Goal: What problem is this eval validating?
# ☐ Input Dataset: Where do the test cases come from?
# ☐ Expected Output: What does "good" look like?
# ☐ Scoring Method: How is success calculated?
# ☐ Target Threshold: What score is acceptable to ship?
# ☐ Owner: Who maintains this eval?
#
# "If you can't fill this out, pause and do that first."
# =============================================================================
